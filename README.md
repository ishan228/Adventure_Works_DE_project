# ğŸ—ï¸ Adventure Works DE Project | Azure Data Engineering Pipeline

This is an end-to-end Data Engineering project built using Microsoft Azure services. The pipeline ingests sales data from the **Adventure Works dataset (Kaggle)**, processes it using **PySpark in Azure Databricks**, stores it in a **data warehouse in Azure Synapse**, and visualizes insights using **Power BI**.

---

## ğŸ”§ Tools & Technologies

- **Azure Data Factory** â€“ Orchestration & Ingestion
- **Azure Databricks** â€“ PySpark for data transformation
- **Azure Synapse Analytics** â€“ Data warehouse & querying
- **Power BI** â€“ Business Intelligence dashboards
- **Azure Blob Storage** â€“ Raw data storage
- **Python, SQL, JSON**

---

## ğŸ“Š Architecture
![Architecture Diagram](images/adw_architecture.drawio.png)




---

## ğŸ“ Folder Structure

```
Adventure_Works_DE_project/
ğŸ—„ï¸ datafactory/
ğŸ—„ï¸   pipelines/
ğŸ—„ï¸   datasets/
ğŸ—„ï¸   linkedservices/
ğŸ—„ï¸ databricks/
ğŸ—„ï¸   notebooks/
ğŸ—„ï¸ synapse/
ğŸ—„ï¸   sql/
ğŸ—„ï¸ dataset/
ğŸ—„ï¸   sample-data.csv
ğŸ—„ï¸ images/
ğŸ—„ï¸   architecture-diagram.png
ğŸ—„ï¸   adf_pipline.png
README.md
```

---

## ğŸ”„ End-to-End Pipeline Overview

### 1ï¸âƒ£ Data Ingestion with Azure Data Factory

- Ingested raw CSV data from Azure Blob Storage.
- Created linked services and datasets to define the source and sink.

### 2ï¸âƒ£ Data Transformation in Azure Databricks

- Read raw data using PySpark.
- Cleaned null values, cast data types, derived fields (e.g., total sales, customer region).
- Exported the processed data as Delta or Parquet to curated layer.

### 3ï¸âƒ£ Data Warehousing in Azure Synapse

- Created fact and dimension tables from processed data.
- Used SQL scripts for table creation and querying.
- Exposed curated data to BI layer via SQL views.

---

## ğŸ“Š Dataset Source

[**Adventure Works Dataset**](https://www.kaggle.com/datasets/ukveteran/adventure-works)

- Sales, customers, products, territories
- Downloaded manually and uploaded to Azure Blob Storage

---

## ğŸ” Note on Secrets

> No real credentials or keys are stored in this repository. All sensitive data (client secrets, keys, connection strings) have been removed or replaced with placeholders.

---

## ğŸ“Œ How to Reproduce (Locally or in Cloud)

> Note: This project is cloud-native and depends on Azure services.

1. Clone the repo
2. Upload your own copy of the dataset to Azure Storage
3. Recreate ADF pipelines using provided JSON
4. Run PySpark scripts in Databricks
5. Use Synapse scripts to create tables
6. Connect Power BI and import the dashboard

---

## ğŸ“· Screenshots

| ADF Pipeline | Databricks Transformation |
| ------------ | ------------------------- | 
|              |                           |           

---

## ğŸ¤ Credits

- Dataset: [Adventure Works â€“ Kaggle](https://www.kaggle.com/datasets/ukveteran/adventure-works)
- Icons: [Azure Architecture Icons](https://learn.microsoft.com/en-us/azure/architecture/icons/)

---



---

## ğŸ“® Contact

If you'd like to collaborate, improve this project, or connect about data roles in Australia ğŸ‡¦ğŸ‡º, feel free to reach out:

**LinkedIn**: [@ishan--gautam](https://www.linkedin.com/in/ishan--gautam/)

